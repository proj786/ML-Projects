{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "EFVNxc91Cxcc",
      "metadata": {
        "id": "EFVNxc91Cxcc"
      },
      "source": [
        "# **Build Regression (Linear,Ridge,Lasso) Models in NumPy Python**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fYQQuHxKC1yW",
      "metadata": {
        "id": "fYQQuHxKC1yW"
      },
      "source": [
        "## **Project Overview**\n",
        "\n",
        "\n",
        "\n",
        "The English Premier League is one of the world's most-watched soccer leagues, with an estimated audience of 12 million people per game.\n",
        "With the substantial financial benefits, all significant teams of EPL are interested in Analytics and AI. Regarding sports analytics, machine learning and artificial intelligence (AI) have become extremely popular. The sports entertainment sector and the relevant stakeholders extensively use sophisticated algorithms to improve earnings and reduce business risk associated with selecting or betting on the wrong players.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![image](https://cdn.pixabay.com/photo/2016/04/15/20/28/football-1331838__340.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "Regression is one of the foundational techniques in Machine Learning. As one of the most well-understood algorithms, linear regression plays a vital role in solving real-life problems.\n",
        "\n",
        "This project explains how linear regression works and how to build various regression models such as linear regression, ridge regression, lasso regression, and decision tree from scratch using the NumPy module."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OH18sTtzwnMQ",
      "metadata": {
        "id": "OH18sTtzwnMQ"
      },
      "source": [
        "## **Who is this notebook intended for?**\n",
        "\n",
        "Regression is one of the foundational techniques in Machine Learning. Being one of the\n",
        "most well-understood algorithms, beginners need some help to understand\n",
        "fundamental terminology related to regression. This is the second notebook out of the three notebooks for this project.\n",
        "\n",
        "<br>\n",
        "\n",
        "![student](https://img.freepik.com/premium-vector/man-working-with-laptop-cartoon-illustration-labour-day-concept-white-isolated-flat-cartoon-style_75802-203.jpg?w=400)\n",
        "\n",
        "This notebook is intended for anyone who wants to understand how to build a decision tree or regression tree from scratch using Numpy. It is suitable for individuals who are new to machine learning and want to gain a deeper understanding of the fundamentals of decision trees, as well as for individuals who have some experience in machine learning but want to learn how to implement decision trees using Numpy.\n",
        "\n",
        "In this notebook, we will walk you through the process of building a regression tree from scratch using Numpy, which will help you gain a deeper understanding of how decision trees work. We provide detailed explanations of each step, including the math behind these models, to ensure that you understand the concepts thoroughly. By the end of this notebook, you will have a good understanding of how to build a decision tree from scratch, which you can apply to other machine learning projects. Additionally, we will compare the results of our implementation with scikit-learn's decision tree to understand how our model performs in comparison to a pre-built tool.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zUD-kOmAyGtj",
      "metadata": {
        "id": "zUD-kOmAyGtj"
      },
      "source": [
        "## **Learning Outcomes**\n",
        "\n",
        "* Understand the importance of preprocessing data and remove correlated features and drop rows with missing data from the dataset.\n",
        "\n",
        "* Use the train-test split function to split the dataset into training and testing sets.\n",
        "\n",
        "* Understanding the concept of decision trees: By building a decision tree from scratch, learners will gain a deeper understanding of how decision trees work, including how they split data based on specific features and how they handle missing values.\n",
        "\n",
        "* Comparing the results of the decision tree built from scratch with scikit-learn's decision tree.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hmowYT-Me4ZU",
      "metadata": {
        "id": "hmowYT-Me4ZU"
      },
      "source": [
        "## **Execution Instructions**\n",
        "<br>\n",
        "\n",
        "### Option 1: Running on your computer locally\n",
        "\n",
        "To run the notebook on your local system set up a [python](https://www.python.org/) environment. Set up the [jupyter notebook](https://jupyter.org/install) with python or by using [anaconda distribution](https://anaconda.org/anaconda/jupyter). Download the notebook and open a jupyter notebook to run the code on local system.\n",
        "\n",
        "The notebook can also be executed by using [Visual Studio Code](https://code.visualstudio.com/), and [PyCharm](https://www.jetbrains.com/pycharm/).\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Option 2: Execution with [ProjectPro Lab](https://www.projectpro.io/user/my-labs)\n",
        "\n",
        "To run the notebook on ProjectPro Lab click on the Open Code Lab button on [project page ](https://www.projectpro.io/project/hackerday-project/project-title/build%20numpy%20regression%20models#sub-about-hackerday).\n",
        "Watch how to run the code using lab [here](https://www.projectpro.io/user/my-labs).\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "### Option 3: Executing with Colab\n",
        "Colab, or \"Collaboratory\", allows you to write and execute Python in your browser, with access to GPUs free of charge and easy sharing.\n",
        "\n",
        "You can run the code using [Google Colab](https://colab.research.google.com/) by uploading the ipython notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0llo4kGzbMS",
      "metadata": {
        "id": "f0llo4kGzbMS"
      },
      "source": [
        "## **Approach**\n",
        "\n",
        "* Importing the required libraries and reading the dataset.\n",
        "* Data pre-processing\n",
        "  * Removing the missing data points\n",
        "  * Dropping categorical variables\n",
        "  * Checking for multi-collinearity and removal of highly correlated features\n",
        "* Creating train and test data by randomly shuffling data\n",
        "* Performing train test split\n",
        "* Model building using NumPy\n",
        "  * Node Regression: Coding from Scratch\n",
        "  * Training Scikit Learn's Decision Tree Model\n",
        "  * Comparing the results of both the models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0uq5Dkfyz-JW",
      "metadata": {
        "id": "0uq5Dkfyz-JW"
      },
      "source": [
        "## **Important Libraries**\n",
        "\n",
        "* **pandas**: pandas is a fast, powerful, flexible, and easy-to-use open-source data analysis and manipulation tool built on top of the Python programming language. Refer to [documentation](https://pandas.pydata.org/) for more information.\n",
        "\n",
        "* **NumPy**: The fundamental package for scientific computing with Python. Fast and versatile, the NumPy vectorization, indexing, and broadcasting concepts are the de-facto standards of array computing today. NumPy offers comprehensive mathematical functions, random number generators, linear algebra routines, Fourier transforms, and more. Refer to [documentation](https://numpy.org/) for more information. pandas and NumPy are together used for most of the data analysis and manipulation in Python.\n",
        "\n",
        "* **scikit-learn**: Simple and efficient tools for predictive data analysis\n",
        "accessible to everybody and reusable in various contexts.\n",
        "It is built on NumPy, SciPy, and matplotlib to support machine learning in Python. Refer to [documentation](https://scikit-learn.org/stable/) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EbnnplKi0F3k",
      "metadata": {
        "id": "EbnnplKi0F3k"
      },
      "source": [
        "## **Install Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oj5XbR8Y0Iws",
      "metadata": {
        "id": "oj5XbR8Y0Iws"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V_K7deligiuv",
      "metadata": {
        "id": "V_K7deligiuv"
      },
      "source": [
        "## **Data Reading from Different Sources**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AQ6OOD345b0-",
      "metadata": {
        "id": "AQ6OOD345b0-"
      },
      "source": [
        "#### **1. Files**\n",
        "\n",
        "In many cases, the data is stored in local system. To read the data from the local system, specify the correct path and filename.\n",
        "* **CSV format**\n",
        "\n",
        "Comma-separated values, also known as CSV, is a specific way to store data in a table structure format.\n",
        "The data used in this project is stored in a CSV file.\n",
        "Click [here](https://www.projectpro.io/project/hackerday-project/project-title/build%20numpy%20regression%20models#sub-hackerday-resources) to download the data used in this project.\n",
        "\n",
        "Use following code to read data from csv file using pandas.\n",
        "```\n",
        "import pandas as pd\n",
        "csv_file_path= \"D:/ProjectPro/Build-Regression-(Linear,Ridge,Lasso)-Models-in-NumPy-Python/data/EPL_Soccer_MLR_LR.csv\"\n",
        "df = pd.read_csv(csv_file_path)\n",
        "```\n",
        "With appropriate csv_file_path, pd.read_csv() function will read the data and store it in df variable.\n",
        "\n",
        "If you get *FileNotFoundError or No such file or directory*, try checking the path provided in the function. It's possible that python is not able to find the file or directory at a given location.\n",
        "\n",
        "<br>\n",
        "\n",
        "* **Public URL**\n",
        "\n",
        "pandas.read_csv() method also works if the data is available on any public URL.\n",
        "In this case, the data is made exclusively available for ProjectPro users [here](https://s3.amazonaws.com/projex.dezyre.com/build-numpy-regression-models/materials/EPL_Soccer_MLR_LR.csv).\n",
        "\n",
        "```\n",
        "import pandas as pd\n",
        "\n",
        "data_url=\"https://s3.amazonaws.com/projex.dezyre.com/build-numpy-regression-models/materials/EPL_Soccer_MLR_LR.csv\"\n",
        "\n",
        "df = pd.read_csv(data_url)\n",
        "\n",
        "```\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4zm6A0V0SkP",
      "metadata": {
        "id": "c4zm6A0V0SkP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-Dlz1f130Upj",
      "metadata": {
        "id": "-Dlz1f130Upj"
      },
      "outputs": [],
      "source": [
        "# Load the data as a data frame by using URL\n",
        "\n",
        "soccer_data_url=\"https://s3.amazonaws.com/projex.dezyre.com/build-numpy-regression-models/materials/EPL_Soccer_MLR_LR.csv\"\n",
        "df = pd.read_csv(soccer_data_url)\n",
        "print(\"ACTUAL DF SHAPE : \", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8LeFQfsE2f71",
      "metadata": {
        "id": "8LeFQfsE2f71"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0tCiNUFqgt0i",
      "metadata": {
        "id": "0tCiNUFqgt0i"
      },
      "source": [
        "## **Data Dictionary**\n",
        "\n",
        "* PlayerName : Player Name\n",
        "\n",
        "* Club : Club of the player\n",
        "  1. MUN:Manchester United F.C.\n",
        "  2. CHE: Chelsea F.C.\n",
        "  3. LIV: Liverpool F.C.\n",
        "\n",
        "* DistanceCovered(InKms): Average Kms distance covered by the player in each game\n",
        "\n",
        "* Goals: Average Goals per match\n",
        "\n",
        "* MinutestoGoalRatio: Minutes\n",
        "\n",
        "* ShotsPerGame: Average shots taken per game\n",
        "\n",
        "* AgentCharges: Agent Fees in h\n",
        "\n",
        "* BMI: Body-Mass index\n",
        "\n",
        "* Cost: Cost of each player in hundread thousand dollars\n",
        "\n",
        "* PreviousClubCost: Previous club cost in hundread thousand dollars\n",
        "\n",
        "* Height: Height of player in cm\n",
        "\n",
        "* Weight: Weight of player in kg\n",
        "\n",
        "* Score: Average score per match\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RI8vlxnI2s-e",
      "metadata": {
        "id": "RI8vlxnI2s-e"
      },
      "source": [
        "## **Data Preprocessing**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bES9foKe2wuw",
      "metadata": {
        "id": "bES9foKe2wuw"
      },
      "source": [
        "### **Handling Missing Values**\n",
        "\n",
        "Handling missing values is an important step in data preprocessing. Missing data can lead to biased results or affect the performance of machine learning models. There are several ways to handle missing values such as:\n",
        "\n",
        "* Dropping missing values: This approach involves dropping the rows or columns that contain missing values. However, this method may lead to loss of information, especially if a large number of rows or columns have missing values.\n",
        "\n",
        "* Imputing missing values: This approach involves filling in the missing values with a value that is either derived from other observations or estimated using statistical methods. Common imputation methods include mean imputation, median imputation, and regression imputation.\n",
        "\n",
        "* Using advanced methods: There are more advanced methods for handling missing data such as multiple imputation, k-nearest neighbors imputation, and matrix completion. These methods can be more accurate than simple imputation methods, but they may also be more computationally intensive.\n",
        "\n",
        "In general, the choice of how to handle missing values will depend on the specific problem and dataset. It is important to carefully consider the impact of missing values on the analysis and choose an appropriate method that balances the need for accurate results with the computational complexity of the method.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2e6cedf",
      "metadata": {
        "id": "e2e6cedf"
      },
      "outputs": [],
      "source": [
        "df.dropna(axis=0, how='all', thresh=None, subset=None, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbec9db8",
      "metadata": {
        "id": "dbec9db8"
      },
      "outputs": [],
      "source": [
        "#dropping categorical columns\n",
        "new_df = df.select_dtypes(['number'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9yf3scqD25Lv",
      "metadata": {
        "id": "9yf3scqD25Lv"
      },
      "source": [
        "### **Finding Correlated Features**\n",
        "\n",
        "In the context of regression models, finding and removing correlated features is an essential step in preprocessing the data. Correlation is a statistical measure that describes how strongly two variables are related to each other. In the case of features, if two features have a high correlation with each other, it means that they provide similar information to the model, which can cause issues such as overfitting, multicollinearity, and reduced interpretability.\n",
        "\n",
        "To find correlated features, we typically calculate a correlation matrix between all pairs of features. We can use tools such as NumPy's corrcoef function or Pandas' corr method to compute the correlation matrix. Once we have the correlation matrix, we can identify highly correlated feature pairs by looking at the absolute values of the correlation coefficients.\n",
        "\n",
        "To know more about Multicollinearity and its affects, please go through the [Linear Regression Model Project in Python for Beginners Part 2 Notebook](https://s3.amazonaws.com/projex.dezyre.com/multiple-linear-regression-project-for-beginners/materials/regression_part2_.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5be6876",
      "metadata": {
        "id": "e5be6876"
      },
      "outputs": [],
      "source": [
        "# The last column (Score) is our dependent variable\n",
        "X = new_df.iloc[:,:-1]\n",
        "y = new_df.iloc[:,-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6289deb",
      "metadata": {
        "id": "f6289deb"
      },
      "outputs": [],
      "source": [
        "#Calculation of Correlated Matrix\n",
        "correlated_features = set()\n",
        "correlation_matrix = X.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "845b1d8d",
      "metadata": {
        "id": "845b1d8d",
        "outputId": "59c91de8-d4c3-4180-fd31-0e46b87503fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Height', 'MinutestoGoalRatio', 'ShotsPerGame', 'Weight'}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Finding correlated columns\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            correlated_features.add(colname)\n",
        "\n",
        "correlated_features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7h-ZqFiQ4VwW",
      "metadata": {
        "id": "7h-ZqFiQ4VwW"
      },
      "source": [
        "## **Train - Test Split**\n",
        "\n",
        "![train-test-split.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "The data points are divided into two datasets, train and test, in a train test split method. The train data is used to train the model, and the model is then used to predict on the test data to see how the model performs on unseen data and whether it is overfitting or underfitting.\n",
        "\n",
        "In this project we are taking 80:20 ratio of train and test data.\n",
        "\n",
        "<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64eDhJ2mT67J",
      "metadata": {
        "id": "64eDhJ2mT67J"
      },
      "source": [
        "### **Underfitting and Overfitting**\n",
        "\n",
        "* **Underfitting**: Underfitting occurs when a statistical model or machine learning algorithm fails to capture the underlying trend of the data, i.e., it performs well on training data but poorly on testing data. Its occurrence merely indicates that our model or method does not adequately suit the data. It frequently occurs when we select a simpler model yet the data contains complicated non-linear patterns or when there is insufficient data to develop a linear model. The obvious approach is to build a complex model or increase the number of linear features in the data.\n",
        "\n",
        "* **Overfitting**: When a statistical model fails to produce correct predictions on testing data, it is said to be overfitted. When a model is trained with a large amount of data, it begins to learn from the noise and incorrect data entries in our data set. It usually occurs when we build a complex model on a simpler dataset.\n",
        "An overfitted model performs well on training data because it has memorized the patterns in the data, but it performs poorly on testing data. An under-fitted model, on the other hand, will perform worse on both datasets because it is unable to capture the trends and patterns underlying the dataset when training.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "345ea467",
      "metadata": {
        "id": "345ea467"
      },
      "outputs": [],
      "source": [
        "# Data Train test split with shuffle as true\n",
        "\n",
        "def shuffle_data(X, y, seed=None):\n",
        "    \"\"\" Random shuffle of the samples in X and y \"\"\"\n",
        "    if seed:\n",
        "        np.random.seed(seed)\n",
        "    idx = np.arange(X.shape[0])\n",
        "    np.random.shuffle(idx)\n",
        "    try:\n",
        "        return X[idx], y[idx]\n",
        "    except:\n",
        "        return X.iloc[idx], y.iloc[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RAlFh7up4d3T",
      "metadata": {
        "id": "RAlFh7up4d3T"
      },
      "source": [
        "#### **Shuffle Data: Code Explanation**\n",
        "\n",
        "The `shuffle_data()` function takes three parameters: `X`,` y`, and `seed`. `X` and `y` are the input features and target variable, respectively, which can be either an `ndarray` or a` DataFrame`. The `seed` parameter is an optional integer that can be used to set the seed for the random number generator.\n",
        "\n",
        "The function first checks if a seed has been provided, and if so, sets the seed for the random number generator. Then, it creates an array of indices that correspond to the samples in `X` and `y` using the `arange()` function from NumPy. Next, it uses the `shuffle()` function from NumPy to randomly shuffle the indices.\n",
        "\n",
        "Finally, the function returns the shuffled input features and target variable. If `X` and `y` are DataFrame objects, the function uses the `iloc[]` indexer to retrieve the shuffled data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vedxQdj74gje",
      "metadata": {
        "id": "vedxQdj74gje"
      },
      "outputs": [],
      "source": [
        "def train_test_split(X, y, test_size=0.5, shuffle=True, seed=None):\n",
        "    \"\"\"\n",
        "    Splits the data into training and test sets.\n",
        "    \"\"\"\n",
        "\n",
        "    if shuffle:\n",
        "        # shuffle the data\n",
        "        X, y = shuffle_data(X, y, seed)\n",
        "\n",
        "    # Split the training data from test data in the ratio specified in test_size\n",
        "    split_i = len(y) - int(len(y) // (1 / test_size))\n",
        "    X_train, X_test = X[:split_i], X[split_i:]\n",
        "    y_train, y_test = y[:split_i], y[split_i:]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cvC3ui4H4snH",
      "metadata": {
        "id": "cvC3ui4H4snH"
      },
      "source": [
        "#### **Train Test Split: Code Explanation**\n",
        "\n",
        "This function takes in the input feature matrix `X` and the target variable `y` and splits the data into training and test sets. The proportion of the data to be included in the test set is specified by the `test_size` parameter, which defaults to 0.5. If `shuffle` is set to True, the data is shuffled before splitting. A random seed can be specified using the `seed` parameter.\n",
        "\n",
        "The `len(y)` returns the number of rows in the target variable `y`. The `//` operator performs floor division, which means that the result of `len(y) // (1 / test_size)` is the number of rows that should be in the test set. The `int` function is used to convert this result to an integer.\n",
        "\n",
        "`split_i` represents the index at which the data should be split, with the first split_i rows being used for training and the remaining rows being used for testing.\n",
        "\n",
        "The code then uses slicing to split the input data `X` into the training and testing sets. The `X[:split_i]` notation selects the first `split_i` rows of the data, while `X[split_i:]` selects the remaining rows. Similarly, the target variable y is split using the same index.\n",
        "\n",
        "The final result of the function is a tuple containing four arrays: X_train, X_test, y_train, and y_test, which represent the input and target variables for the training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "301b4767",
      "metadata": {
        "id": "301b4767"
      },
      "outputs": [],
      "source": [
        "# # finally using the train_test_split() function to create a test set of 20% of the data with seed = 42\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2zZ_AhSA4y0A",
      "metadata": {
        "id": "2zZ_AhSA4y0A"
      },
      "source": [
        "### **Did you know - I**\n",
        "\n",
        "Did you know that using a seed in randomization can be important for reproducibility of results? When a seed is set, the random numbers generated by the computer are deterministic, meaning they will always be the same. This can be helpful when testing and comparing different models or algorithms, as it ensures that the results are consistent and not affected by random chance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214e4f4e",
      "metadata": {
        "id": "214e4f4e"
      },
      "outputs": [],
      "source": [
        "# Dropping correlated features\n",
        "X_train.drop(columns=correlated_features, axis=1, inplace=True)\n",
        "X_test.drop(columns=correlated_features, axis=1, inplace=True)\n",
        "X.drop(columns=correlated_features, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GdmY31GI42Iv",
      "metadata": {
        "id": "GdmY31GI42Iv"
      },
      "source": [
        "## **Regression with Decision Trees**\n",
        "\n",
        "\n",
        "Decision trees are a popular machine learning algorithm that can be used for both classification and regression tasks. In this explanation, we'll focus on how decision trees work for regression tasks.\n",
        "\n",
        "A decision tree is a tree-like model that is used to make decisions based on a set of rules. The tree is made up of nodes and branches, where each node represents a decision and each branch represents a possible outcome. The top node in the tree is called the root, and the nodes at the bottom of the tree are called leaves.\n",
        "\n",
        "\n",
        "Here are some important parameters:\n",
        "\n",
        "* criterion: The criterion parameter specifies the function used to measure the quality of a split. Two common options are \"gini\" and \"entropy\". The \"gini\" criterion is used to minimize the probability of misclassification, while the \"entropy\" criterion is used to maximize information gain.\n",
        "\n",
        "* splitter: The splitter parameter specifies the strategy used to split the data at each node. Two common options are \"best\" and \"random\". The \"best\" splitter chooses the best split based on the selected criterion, while the \"random\" splitter chooses the best random split.\n",
        "\n",
        "* max_depth: The max_depth parameter specifies the maximum depth of the tree. If not specified, the tree will continue to split until all leaves are pure, or until all leaves contain fewer than the minimum number of samples required to split.\n",
        "\n",
        "* min_samples_split: The min_samples_split parameter specifies the minimum number of samples required to split an internal node. If a node has fewer samples than this parameter, it will not be split.\n",
        "\n",
        "* min_samples_leaf: The min_samples_leaf parameter specifies the minimum number of samples required to be at a leaf node. If a leaf node has fewer samples than this parameter, it will be pruned.\n",
        "\n",
        "* max_features: The max_features parameter specifies the maximum number of features that are considered when looking for the best split. This can help to reduce overfitting by only considering a subset of the available features.\n",
        "\n",
        "* max_leaf_nodes: The max_leaf_nodes parameter specifies the maximum number of leaf nodes that are allowed in the tree. If not specified, the tree will grow until all leaves are pure or until all leaves contain fewer than the minimum number of samples required to split.\n",
        "\n",
        "#### **How does a Decision Tree predicts?**\n",
        "\n",
        "In regression tasks, decision trees predict the value of a continuous target variable based on the values of several input features. The prediction process works by recursively partitioning the feature space into smaller regions based on the values of the input features. Each region is associated with a prediction value that is the mean (or median) of the target variable values in that region.\n",
        "\n",
        "The process of partitioning the feature space begins with the root node of the tree, which represents the entire feature space. The root node is split into two child nodes based on a feature and a threshold value. The feature and threshold value are chosen such that the split results in the maximum reduction of a cost function, which measures the error or variance of the target variable in the current region. This process is repeated recursively for each child node until a stopping criterion is met, such as a maximum depth of the tree or a minimum number of samples in a leaf node.\n",
        "\n",
        "Once the decision tree is trained, making predictions on new data involves traversing the tree from the root node to a leaf node that corresponds to the region in which the new data falls. The prediction value for that region is then returned as the final prediction. The decision tree's ability to predict the target variable value for new data depends on the quality of the splits made during training and the stopping criterion used.\n",
        "\n",
        "\n",
        "#### **Decision Tree and its Cost Functions**\n",
        "\n",
        "In a decision tree, the choice of the cost function (also known as the splitting criterion) determines how the algorithm splits the data at each internal node. The goal of the cost function is to select the feature and split point that result in the best separation of the data, based on some evaluation metric.\n",
        "Some of the commonly used cost functions include:\n",
        "\n",
        "* Mean Squared Error (MSE): This is the most commonly used cost function for regression problems. It measures the average squared difference between the predicted and actual values in a given region of the feature space. The formula for MSE is:\n",
        "$$MSE= \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat y_i)^2$$\n",
        "where $n$ is the number of samples, $y_i$ is the actual target value for the ith sample, and $\\hat y_i$ is the predicted value for the ith sample.\n",
        "\n",
        "\n",
        "* Gini impurity: This is a cost function used in classification problems. It measures the probability of misclassifying a randomly chosen sample from a region of the feature space. The formula for Gini impurity is:\n",
        "$$G = 1 - \\sum_{i=1}^{n} (p_i)^2$$\n",
        "where $p_i$ is the probability of the $i$-th class in the region.\n",
        "\n",
        "* Information gain: This is another cost function used in classification problems. It measures the reduction in entropy (uncertainty) of the class labels in a given region of the feature space. The formula for information gain is:\n",
        "$$IG = H(parent) - \\sum_{i=1}^{k} w_i H(child_i)$$\n",
        "where $H$ is the entropy of a region, $k$ is the number of child nodes, and $w_i$ is the proportion of the total number of samples in the parent node that belong to child node $i$.\n",
        "\n",
        "\n",
        "The choice of cost function depends on the type of problem, the structure of the data, and the performance goals. MSE and MAE are typically used for regression problems, while Gini impurity and information gain are typically used for classification problems.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Here's an example of a simple decision tree for a regression task:\n",
        "\n",
        "\n",
        "```        \n",
        "           X < 0.5\n",
        "         /         \\\n",
        "     1.2           0.8\n",
        "    /   \\         /   \\\n",
        "  0.9   1.5     1.1   0.7\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "In this example, we're trying to predict a continuous target variable based on a single input feature, X. The tree has three levels, with the root node splitting the data based on whether X is less than 0.5. If X < 0.5, the predicted value is 1.2, otherwise it's 0.8.\n",
        "\n",
        "\n",
        "\n",
        "One advantage of decision trees is that they are easy to interpret and visualize, since the rules for making predictions are explicit and can be shown in a tree structure. However, they are also prone to overfitting if the tree is too deep or if there are too many features, which can result in poor generalization to new data.\n",
        "\n",
        "\n",
        "**Decision Tree and Overfitting**\n",
        "\n",
        "Decision trees are prone to overfitting  due to their ability to capture even the smallest nuances and noise in the training data. Decision trees aim to split the data into smaller and more homogeneous subsets based on the value of a single feature at a time. This process continues until the subsets reach a user-defined maximum depth or no further split can be made based on the given criteria. The issue with this approach is that the tree can continue to grow, even to the point of having a separate leaf node for each training data point.\n",
        "\n",
        "Decision trees have the tendency to capture every detail in the training data, which may include noise and outliers, resulting in a tree that perfectly fits the training data but generalizes poorly to new, unseen data. This is because the tree is designed to perfectly fit all samples in the training dataset, even if it means capturing noise or irrelevant information.\n",
        "\n",
        "Moreover, in regression, the split criterion used in decision trees is typically based on reducing the mean squared error (MSE) or other related metrics. This measure ensures that each split makes the maximum reduction in the MSE of the target variable. This can result in a tree that is too complex, with too many nodes and leaves, as the algorithm exhaustively searches for the best split at every node, regardless of whether the improvement is significant or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FvjcUueJmQ1-",
      "metadata": {
        "id": "FvjcUueJmQ1-"
      },
      "source": [
        "### **Ways to reduce Overfitting in a Decision Tree**\n",
        "\n",
        "There are several ways to optimize the structure of a decision tree to reduce overfitting while maintaining or improving model accuracy:\n",
        "\n",
        "* Pruning: Pruning is a technique that reduces the size of the decision tree by removing branches that do not provide any additional information. There are two types of pruning: pre-pruning and post-pruning. Pre-pruning involves stopping the tree's growth early, while post-pruning involves removing unnecessary branches after the tree has been built.\n",
        "\n",
        "* Minimum number of samples: The minimum number of samples required to split a node can be increased to prevent the tree from splitting too early, which can lead to overfitting.\n",
        "\n",
        "* Maximum depth: The maximum depth of the tree can be limited to prevent the tree from becoming too complex and overfitting the training data.\n",
        "\n",
        "* Regularization: Decision trees can be regularized using techniques such as L1 or L2 regularization, which add a penalty term to the cost function to prevent the tree from overfitting.\n",
        "\n",
        "* Ensemble methods: Ensemble methods, such as Random Forest and Gradient Boosted Trees, can be used to combine multiple decision trees and reduce overfitting. These methods create multiple decision trees and aggregate their results to make more accurate predictions.\n",
        "\n",
        "Overall, it is important to balance the complexity of the decision tree with the accuracy of the model. By optimizing the structure of the tree, it is possible to reduce overfitting while maintaining or improving model accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qYTd30RtpNA_",
      "metadata": {
        "id": "qYTd30RtpNA_"
      },
      "source": [
        "### **Think about it - I**\n",
        "\n",
        "Imagine you are a data scientist trying to build a predictive model to classify customer churn for a telecommunication company. You have a large dataset with many features, but after trying different models, you are still not getting the accuracy you need. You start wondering: what if I combined the predictions of several models to get a better result? How could I do that? What are the potential benefits and drawbacks of this approach?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OBpwWgZmqHX6",
      "metadata": {
        "id": "OBpwWgZmqHX6"
      },
      "source": [
        "**Before we dive into building our node regression or decision tree model from scratch with numpy, let's review a few important concepts of object-oriented programming in Python. If you're already familiar with these concepts, feel free to skip ahead to the next section.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nqJ6GMjTqQwY",
      "metadata": {
        "id": "nqJ6GMjTqQwY"
      },
      "source": [
        "## **Basics of Object-Oriented Programming in Python**\n",
        "\n",
        "Object-oriented programming (OOP) is a programming paradigm based on the concept of \"objects\" which contain both data and methods that operate on that data. Python is an object-oriented language, which means it has built-in support for OOP concepts such as encapsulation, inheritance, and polymorphism.\n",
        "\n",
        "1. Encapsulation: This concept refers to the practice of bundling data and the  ethods that operate on that data into a single unit, which can then be treated as a black box. The data can be accessed and manipulated only through the methods of the object. In Python, encapsulation is achieved through the use of class and object instances.\n",
        "  * Class: A class is a blueprint for creating objects that defines a set of attributes and methods that are common to all objects of that class.\n",
        "\n",
        "  * Object: An object is an instance of a class that has its own attributes and methods.\n",
        "\n",
        "  * Init method: The` __init__()` method is a special method that is automatically called when an object is created from a class. This method initializes the attributes of the class and can take parameters that are passed when the object is created.\n",
        "\n",
        "  * Self: The `self` keyword refers to the object itself and is used to access its attributes and methods within a class.\n",
        "\n",
        "  * Methods: Methods are functions that are defined inside a class and are used to perform certain actions on the object.\n",
        "\n",
        "2. Inheritance: This concept refers to the ability of a class to inherit the properties and methods of another class. The class that is being inherited from is called the parent or base class, and the class that is inheriting from it is called the child or derived class.\n",
        "\n",
        "3. Polymorphism: This concept refers to the ability of objects of different classes to be treated as if they were of the same class. This is typically achieved through the use of inheritance and method overriding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x2cGthinqUkW",
      "metadata": {
        "id": "x2cGthinqUkW"
      },
      "outputs": [],
      "source": [
        "class BankAccount:\n",
        "    def __init__(self, name, balance):\n",
        "        self.name = name\n",
        "        self.balance = balance\n",
        "\n",
        "    def deposit(self, amount):\n",
        "        self.balance += amount\n",
        "\n",
        "    def withdraw(self, amount):\n",
        "        if self.balance - amount < 0:\n",
        "            print(\"Insufficient funds\")\n",
        "        else:\n",
        "            self.balance -= amount\n",
        "\n",
        "account = BankAccount(\"UserName\", 500)\n",
        "account.deposit(100)\n",
        "account.withdraw(50)\n",
        "print(account.balance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2jcQZdL6qXnU",
      "metadata": {
        "id": "2jcQZdL6qXnU"
      },
      "source": [
        "In this example, we define a `BankAccount` class that encapsulates the data (name and balance) and the methods (deposit and withdraw) that operate on that data. We create an instance of the class and use its methods to deposit and withdraw funds.\n",
        "We define the class with `__init__()` method, in this example we are defining a bank account with a user's name and the balance. While creating methods such as `deposit()`, we first call `self.balance` which identifies the account's or that class's balance and then we update it.\n",
        "Once class and it's features (data and methods) are defined, we can generate multiple objects of it. For example, here, we create an object called `account`.\n",
        "So when we run the code `account.deposit(100)`, it updates the previous balance which was 500 as we initialized the object with 500 and adds 100.\n",
        "We can check the balance with the code `print(account.balance)`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RJ5xk0_-qTwd",
      "metadata": {
        "id": "RJ5xk0_-qTwd"
      },
      "source": [
        "In this project, we will primarily be using the concept of encapsulation in object-oriented programming. However, feel free to explore and experiment with other concepts such as inheritance and polymorphism to enhance your understanding of Python OOP. These concepts can be useful for building more complex models or working with larger codebases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23210700",
      "metadata": {
        "id": "23210700"
      },
      "outputs": [],
      "source": [
        "class NodeRegression():\n",
        "    \"\"\"\n",
        "    Class to grow a regression decision tree\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        Y: list,\n",
        "        X: pd.DataFrame,\n",
        "        min_samples_split=None,\n",
        "        max_depth=None,\n",
        "        depth=None,\n",
        "        node_type=None,\n",
        "        rule=None\n",
        "    ):\n",
        "        # Saving the data to the node\n",
        "        self.Y = Y\n",
        "        self.X = X\n",
        "\n",
        "        # Saving the hyper parameters\n",
        "        self.min_samples_split = min_samples_split if min_samples_split else 20\n",
        "        self.max_depth = max_depth if max_depth else 5\n",
        "\n",
        "        # Default current depth of node\n",
        "        self.depth = depth if depth else 0\n",
        "\n",
        "        # Extracting all the features\n",
        "        self.features = list(self.X.columns)\n",
        "\n",
        "        # Type of node\n",
        "        self.node_type = node_type if node_type else 'root'\n",
        "\n",
        "        # Rule for spliting\n",
        "        self.rule = rule if rule else \"\" #which feature is used for splitting\n",
        "\n",
        "        # Getting the mean of Y\n",
        "        self.ymean = np.mean(Y)\n",
        "\n",
        "        # Getting the residuals\n",
        "        self.residuals = self.Y - self.ymean\n",
        "\n",
        "        # Calculating the mse of the node\n",
        "        self.mse = self.get_mse(Y, self.ymean)\n",
        "\n",
        "        # Saving the number of observations in the node\n",
        "        self.n = len(Y)\n",
        "\n",
        "        # Initiating the left and right nodes as empty nodes\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "\n",
        "        # Default values for splits\n",
        "        self.best_feature = None\n",
        "        self.best_value = None\n",
        "\n",
        "\n",
        "    def get_mse(self, y_true, y_pred) -> float:\n",
        "        \"\"\"\n",
        "        Method to calculate the mean squared error\n",
        "        \"\"\"\n",
        "        return np.mean((y_true - y_pred) ** 2)\n",
        "\n",
        "\n",
        "    def ma(self, x: np.array, window: int) -> np.array:\n",
        "        \"\"\"\n",
        "        Calculates the moving average of the given list.\n",
        "        \"\"\"\n",
        "        return np.convolve(x, np.ones(window), 'valid') / window\n",
        "\n",
        "    def best_split(self) -> tuple:\n",
        "        \"\"\"\n",
        "        Given the X features and Y targets calculates the best split\n",
        "        for a decision tree\n",
        "        \"\"\"\n",
        "        # Creating a dataset for spliting\n",
        "        df = self.X.copy()\n",
        "        df['Y'] = self.Y\n",
        "\n",
        "        # Getting the GINI impurity/mse for the base input\n",
        "        mse_base = self.mse\n",
        "\n",
        "        # Finding which split yields the best GINI gain\n",
        "        #max_gain = 0\n",
        "\n",
        "        # Default best feature and split\n",
        "        best_feature = None\n",
        "        best_value = None\n",
        "\n",
        "        for feature in self.features:\n",
        "            # Droping missing values\n",
        "            Xdf = df.dropna().sort_values(feature)\n",
        "\n",
        "            # Sorting the values and getting the rolling average\n",
        "            xmeans = self.ma(Xdf[feature].unique(), 2)\n",
        "\n",
        "            for value in xmeans:\n",
        "                # Getting the left and right ys\n",
        "                left_y = Xdf[Xdf[feature]<value]['Y'].values\n",
        "                right_y = Xdf[Xdf[feature]>=value]['Y'].values\n",
        "\n",
        "                # Getting the means\n",
        "                left_mean = np.mean(left_y)\n",
        "                right_mean = np.mean(right_y)\n",
        "\n",
        "                # Getting the left and right residuals\n",
        "                res_left = left_y - left_mean\n",
        "                res_right = right_y - right_mean\n",
        "\n",
        "                # Concatenating the residuals\n",
        "                r = np.concatenate((res_left, res_right), axis=None)\n",
        "\n",
        "                # Calculating the mse\n",
        "                n = len(r)\n",
        "                r = r ** 2\n",
        "                r = np.sum(r)\n",
        "                mse_split = r / n\n",
        "\n",
        "                # Checking if this is the best split so far\n",
        "                if mse_split < mse_base:\n",
        "                    best_feature = feature\n",
        "                    best_value = value\n",
        "\n",
        "                    # Setting the best gain to the current one\n",
        "                    mse_base = mse_split\n",
        "\n",
        "        return (best_feature, best_value)\n",
        "\n",
        "\n",
        "    # Growing tree recursively\n",
        "    def grow_tree(self):\n",
        "        \"\"\"\n",
        "        Recursive method to create the decision tree\n",
        "        \"\"\"\n",
        "        # Making a df from the data\n",
        "        df = self.X.copy()\n",
        "        df['Y'] = self.Y\n",
        "\n",
        "        # If there is GINI to be gained, we split further\n",
        "        if (self.depth < self.max_depth) and (self.n >= self.min_samples_split):\n",
        "\n",
        "            # Getting the best split\n",
        "            best_feature, best_value = self.best_split()\n",
        "\n",
        "            if best_feature is not None:\n",
        "                # Saving the best split to the current node\n",
        "                self.best_feature = best_feature\n",
        "                self.best_value = best_value\n",
        "\n",
        "                # Getting the left and right nodes\n",
        "                left_df, right_df = df[df[best_feature]<=best_value].copy(), df[df[best_feature]>best_value].copy()\n",
        "\n",
        "                # Creating the left and right nodes\n",
        "                left = NodeRegression(\n",
        "                    left_df['Y'].values.tolist(),\n",
        "                    left_df[self.features],\n",
        "                    depth=self.depth + 1,\n",
        "                    max_depth=self.max_depth,\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    node_type='left_node',\n",
        "                    rule=f\"{best_feature} <= {round(best_value, 3)}\"\n",
        "                    )\n",
        "\n",
        "                self.left = left\n",
        "                self.left.grow_tree()\n",
        "\n",
        "                right = NodeRegression(\n",
        "                    right_df['Y'].values.tolist(),\n",
        "                    right_df[self.features],\n",
        "                    depth=self.depth + 1,\n",
        "                    max_depth=self.max_depth,\n",
        "                    min_samples_split=self.min_samples_split,\n",
        "                    node_type='right_node',\n",
        "                    rule=f\"{best_feature} > {round(best_value, 3)}\"\n",
        "                    )\n",
        "\n",
        "                self.right = right\n",
        "                self.right.grow_tree()\n",
        "\n",
        "    def print_info(self, width=4):\n",
        "        \"\"\"\n",
        "        Method to print the infromation about the tree\n",
        "        \"\"\"\n",
        "        # Defining the number of spaces\n",
        "        const = int(self.depth * width ** 1.5)\n",
        "        spaces = \"-\" * const\n",
        "\n",
        "        if self.node_type == 'root':\n",
        "            print(\"Root\")\n",
        "        else:\n",
        "            print(f\"|{spaces} Split rule: {self.rule}\")\n",
        "        print(f\"{' ' * const}   | MSE of the node: {round(self.mse, 2)}\")\n",
        "        print(f\"{' ' * const}   | Count of observations in node: {self.n}\")\n",
        "        print(f\"{' ' * const}   | Prediction of node: {round(self.ymean, 3)}\")\n",
        "\n",
        "\n",
        "    # Displaying the tree structure\n",
        "    def print_tree(self):\n",
        "        \"\"\"\n",
        "        Prints the whole tree from the current node to the bottom\n",
        "        \"\"\"\n",
        "        self.print_info()\n",
        "\n",
        "        if self.left is not None:\n",
        "            self.left.print_tree()\n",
        "\n",
        "        if self.right is not None:\n",
        "            self.right.print_tree()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZBtYeukP7gRl",
      "metadata": {
        "id": "ZBtYeukP7gRl"
      },
      "source": [
        "### **Node Regression: Code Explanation**\n",
        "\n",
        "#### `__init()__`\n",
        "The code contains a class called NodeRegression, which is used to grow a regression decision tree. This class has several methods, including` __init__()`, which initializes all the attributes of the class, including `Y` and `X`, which are the target and feature variables, respectively. It also has attributes like `min_samples_split`, `max_depth`, `depth`, `node_type`, `rule`, and `features` which are the various parameters to control the growth of the decision tree.\n",
        "\n",
        "#### `get_mse()`\n",
        "The get_mse() method calculates the mean squared error (MSE) of the regression model.\n",
        "\n",
        "#### `ma()`\n",
        "The ma() method calculates the moving average of the given list.\n",
        "\n",
        "\n",
        "#### `best_split()`\n",
        "The best_split() method is used to calculate the best split for a decision tree.\n",
        "The first step is to create a dataframe df for splitting. This dataframe is created by copying the X feature set and adding the Y targets as a new column to the dataframe.\n",
        "\n",
        "The next step is to calculate the mean squared error mse_base for the base input, i.e., for the current node.\n",
        "The loop over features and values is then performed to find the best split. For each feature, the missing values are dropped, and the rows are sorted by the feature values. The ma method is then called to calculate the moving averages of the feature values.\n",
        "The ma method calculates the moving average of the feature values using a window size of 2. This is done because we are looking for a split point between two adjacent feature values, and the moving average helps to smooth out the feature values and identify a point of change.\n",
        "The for loop then iterates over the moving averages and for each moving average, the left and right Y values are calculated. The left Y values are those that have a feature value less than the moving average, while the right Y values are those that have a feature value greater than or equal to the moving average.\n",
        "The mean Y values for the left and right sides are then calculated, and the residuals are obtained by subtracting the mean from each of the Y values. The residuals for the left and right sides are then concatenated to form a single array.\n",
        "\n",
        "The mean squared error (MSE) of the split is then calculated as the sum of the squared residuals divided by the number of residuals. The MSE for the split is then compared to the mse_base calculated earlier, and if the MSE for the split is less than the mse_base, then the feature and value are updated as the best feature and split so far, and the mse_base is updated to the MSE for the split.\n",
        "\n",
        "After iterating over all features and values, the method returns the best feature and value as a tuple. These are the parameters that will be used to split the current node into two new nodes.\n",
        "\n",
        "\n",
        "#### `grow_tree()`\n",
        "\n",
        "\n",
        "This code is part of a recursive algorithm to create a decision tree using the node regression approach.\n",
        "We start by creating a dataframe df from the input features self.X and output self.Y to use for splitting.\n",
        "\n",
        "Then, we check if we can gain any more information by splitting the current node. If the current depth is less than the maximum depth and the number of samples in the current node is greater than or equal to the minimum number of samples required to split, we try to find the best feature and value to split the data.\n",
        "\n",
        "We find the best split by calling the best_split() method.\n",
        "If a valid split is found, we set the best_feature and best_value properties of the current node to the best feature and value.\n",
        "\n",
        "Next, we split the data into left and right nodes based on the best_feature and best_value.\n",
        "\n",
        "We then create new NodeRegression instances for the left and right nodes with the output values of the left and right nodes and the input features of the current node.\n",
        "We set the depth, max_depth, min_samples_split, node_type, and rule properties of each new node and add them to the left and right properties of the current node.\n",
        "\n",
        "We then recursively call the grow_tree() method for each of the new nodes.\n",
        "The recursion stops when we cannot gain any more information by splitting (we got lowest error possible) or when the current depth equals the maximum depth.\n",
        "\n",
        "#### `print_info() and print_tree()`\n",
        "\n",
        "The methods print_info() and print_tree() are used to print information about the tree structure and values in the nodes of the tree.\n",
        "\n",
        "The print_info() method prints the information about a single node, including the split rule, mean squared error (MSE) of the node, number of observations in the node, and the predicted value for the node. The print_tree() method recursively prints the information for each node in the tree from the current node to the bottom.\n",
        "\n",
        "These methods are used to provide visibility into the structure and values in the decision tree model, allowing users to see the splits, MSE, and predicted values for each node in the tree. This can be useful for understanding how the model is making decisions, debugging issues, or simply visualizing the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b7d9bb0",
      "metadata": {
        "id": "9b7d9bb0"
      },
      "outputs": [],
      "source": [
        "# Creation of root node\n",
        "root = NodeRegression(y_train, X_train, max_depth=2, min_samples_split=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1df531a5",
      "metadata": {
        "id": "1df531a5",
        "outputId": "47281a0d-1c21-46f9-976e-47e6df49d20a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<__main__.NodeRegression at 0x1ea0c2d9ee0>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db8135e",
      "metadata": {
        "id": "1db8135e"
      },
      "outputs": [],
      "source": [
        "# growing the tree recursively\n",
        "root.grow_tree()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42061113",
      "metadata": {
        "id": "42061113",
        "outputId": "1824074b-83a6-4f32-9bd0-191dfbb3f567"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root\n",
            "   | MSE of the node: 35.87\n",
            "   | Count of observations in node: 162\n",
            "   | Prediction of node: 13.587\n",
            "|-------- Split rule: Cost <= 68.05\n",
            "           | MSE of the node: 5.33\n",
            "           | Count of observations in node: 93\n",
            "           | Prediction of node: 9.187\n",
            "|---------------- Split rule: Cost <= 44.65\n",
            "                   | MSE of the node: 1.8\n",
            "                   | Count of observations in node: 44\n",
            "                   | Prediction of node: 7.431\n",
            "|---------------- Split rule: Cost > 44.65\n",
            "                   | MSE of the node: 3.24\n",
            "                   | Count of observations in node: 49\n",
            "                   | Prediction of node: 10.764\n",
            "|-------- Split rule: Cost > 68.05\n",
            "           | MSE of the node: 15.78\n",
            "           | Count of observations in node: 69\n",
            "           | Prediction of node: 19.516\n",
            "|---------------- Split rule: Cost <= 109.3\n",
            "                   | MSE of the node: 6.13\n",
            "                   | Count of observations in node: 50\n",
            "                   | Prediction of node: 17.8\n",
            "|---------------- Split rule: Cost > 109.3\n",
            "                   | MSE of the node: 13.03\n",
            "                   | Count of observations in node: 19\n",
            "                   | Prediction of node: 24.032\n"
          ]
        }
      ],
      "source": [
        "# Printing tree\n",
        "root.print_tree()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02de33bb",
      "metadata": {
        "id": "02de33bb"
      },
      "outputs": [],
      "source": [
        "data_path = \"./EPL_Soccer_MLR_LR.csv\"\n",
        "\n",
        "df = pd.read_csv(data_path)\n",
        "\n",
        "df.dropna(axis=0, how='all', thresh=None, subset=None, inplace=True)\n",
        "\n",
        "#dropping categorical columns\n",
        "new_df = df.select_dtypes(['number'])\n",
        "\n",
        "# The last column (Score) is our dependent variable\n",
        "X = new_df.iloc[:,:-1]\n",
        "y = new_df.iloc[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, seed=42)\n",
        "\n",
        "correlated_features = set()\n",
        "correlation_matrix = X.corr()\n",
        "\n",
        "\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
        "            colname = correlation_matrix.columns[i]\n",
        "            correlated_features.add(colname)\n",
        "\n",
        "X_train.drop(columns=correlated_features, axis=1, inplace=True)\n",
        "X_test.drop(columns=correlated_features, axis=1, inplace=True)\n",
        "X.drop(columns=correlated_features, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ZMfKeoDYMS2",
      "metadata": {
        "id": "7ZMfKeoDYMS2"
      },
      "source": [
        "Now we will be comparing the results using the Sklearn's Decision Tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ceb47e",
      "metadata": {
        "id": "a3ceb47e",
        "outputId": "e294b770-2193-44f6-e4b8-ba6156842ba3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.46196735, 0.74246175, 0.87827764, 0.73471287, 0.82410885])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=0)\n",
        "acc = cross_val_score(regressor, X, y, cv=5)\n",
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d45b7eb9",
      "metadata": {
        "id": "d45b7eb9",
        "outputId": "ec34b08c-c21d-4b44-85ad-78c7efb4a9bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of model :  0.728305692\n"
          ]
        }
      ],
      "source": [
        "print(\"Accuracy of model : \", np.mean(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "941bd052",
      "metadata": {
        "id": "941bd052",
        "outputId": "f34a0624-92e4-4c4d-8830-c9792719d7ff"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DistanceCovered(InKms)</th>\n",
              "      <th>Goals</th>\n",
              "      <th>AgentCharges</th>\n",
              "      <th>BMI</th>\n",
              "      <th>Cost</th>\n",
              "      <th>PreviousClubCost</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>4.66</td>\n",
              "      <td>6.4</td>\n",
              "      <td>109.0</td>\n",
              "      <td>18.37</td>\n",
              "      <td>38.2</td>\n",
              "      <td>41.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>4.36</td>\n",
              "      <td>5.8</td>\n",
              "      <td>29.0</td>\n",
              "      <td>21.86</td>\n",
              "      <td>99.9</td>\n",
              "      <td>56.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>4.87</td>\n",
              "      <td>6.4</td>\n",
              "      <td>64.0</td>\n",
              "      <td>20.17</td>\n",
              "      <td>99.8</td>\n",
              "      <td>52.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>5.48</td>\n",
              "      <td>4.6</td>\n",
              "      <td>132.0</td>\n",
              "      <td>32.52</td>\n",
              "      <td>55.7</td>\n",
              "      <td>102.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>5.29</td>\n",
              "      <td>12.7</td>\n",
              "      <td>124.0</td>\n",
              "      <td>23.38</td>\n",
              "      <td>75.9</td>\n",
              "      <td>74.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>5.59</td>\n",
              "      <td>7.9</td>\n",
              "      <td>220.0</td>\n",
              "      <td>23.55</td>\n",
              "      <td>41.9</td>\n",
              "      <td>63.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>5.00</td>\n",
              "      <td>6.7</td>\n",
              "      <td>72.0</td>\n",
              "      <td>24.64</td>\n",
              "      <td>49.6</td>\n",
              "      <td>79.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>4.51</td>\n",
              "      <td>8.3</td>\n",
              "      <td>34.0</td>\n",
              "      <td>21.27</td>\n",
              "      <td>69.9</td>\n",
              "      <td>56.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>4.77</td>\n",
              "      <td>7.1</td>\n",
              "      <td>40.0</td>\n",
              "      <td>26.85</td>\n",
              "      <td>103.6</td>\n",
              "      <td>66.85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>4.96</td>\n",
              "      <td>8.3</td>\n",
              "      <td>141.0</td>\n",
              "      <td>33.73</td>\n",
              "      <td>113.5</td>\n",
              "      <td>89.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>162 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     DistanceCovered(InKms)  Goals  AgentCharges    BMI   Cost  \\\n",
              "95                     4.66    6.4         109.0  18.37   38.2   \n",
              "15                     4.36    5.8          29.0  21.86   99.9   \n",
              "30                     4.87    6.4          64.0  20.17   99.8   \n",
              "159                    5.48    4.6         132.0  32.52   55.7   \n",
              "186                    5.29   12.7         124.0  23.38   75.9   \n",
              "..                      ...    ...           ...    ...    ...   \n",
              "173                    5.59    7.9         220.0  23.55   41.9   \n",
              "131                    5.00    6.7          72.0  24.64   49.6   \n",
              "17                     4.51    8.3          34.0  21.27   69.9   \n",
              "72                     4.77    7.1          40.0  26.85  103.6   \n",
              "177                    4.96    8.3         141.0  33.73  113.5   \n",
              "\n",
              "     PreviousClubCost  \n",
              "95              41.93  \n",
              "15              56.52  \n",
              "30              52.72  \n",
              "159            102.00  \n",
              "186             74.00  \n",
              "..                ...  \n",
              "173             63.00  \n",
              "131             79.00  \n",
              "17              56.31  \n",
              "72              66.85  \n",
              "177             89.00  \n",
              "\n",
              "[162 rows x 6 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d3ee7e1",
      "metadata": {
        "id": "7d3ee7e1"
      },
      "outputs": [],
      "source": [
        "# cost < 44.65, 45 nodes, 7.39 score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc69b38",
      "metadata": {
        "id": "edc69b38"
      },
      "outputs": [],
      "source": [
        "fil_df = new_df[new_df[\"Cost\"]<=44.65]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "680ad9f6",
      "metadata": {
        "id": "680ad9f6",
        "outputId": "b1472d87-7cc7-4436-ffba-3deccaf36b5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(54, 11)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fil_df.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d59584",
      "metadata": {
        "id": "02d59584",
        "outputId": "06e870cd-3725-4341-987e-1c3685a3a31d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7.292777777777779"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(fil_df[\"Score\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lMgLSlxDa1-i",
      "metadata": {
        "id": "lMgLSlxDa1-i"
      },
      "source": [
        "## **Try-it-out - I**\n",
        "\n",
        "\n",
        "* Implement a mechanism to prune the decision tree by setting a minimum threshold for the improvement in the mean squared error (MSE) after a split. The code currently only stops splitting when there is no further gain in MSE. By implementing tree pruning, you can simplify the tree and avoid overfitting to the training data.\n",
        "\n",
        "* In the current code, the decision tree is built using MSE as the impurity metric. However, there are other metrics that can be used, such as mean absolute error (MAE) or coefficient of determination ($R^2$). Implement the decision tree using a different impurity metric and compare the performance of the model.\n",
        "\n",
        "* The current implementation of the decision tree only works with continuous variables. Modify the code to handle categorical variables as well.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zl0BjffXeYeo",
      "metadata": {
        "id": "zl0BjffXeYeo"
      },
      "source": [
        "## **Regression in Real Life**\n",
        "\n",
        "There are many real-world applications of regression. Exploring some of the best Regression real-world applications will help us comprehend the concept more clearly.\n",
        "\n",
        "* Real estate agencies build regression models to predict the selling price of a house based on various features such as square footage, number of bedrooms, and location. They collect data on recently sold houses and their prices and use this data to accurately predict the selling price of a house given its features. Here's another project that solves the same business problem, [Build Regression Models in Python for House Price Prediction](https://www.projectpro.io/project-use-case/house-price-prediction-project-using-machine-learning-regression).\n",
        "\n",
        "* For professional sports teams, analysts use linear regression to gauge the impact of various training schedules on player performance. For instance, data scientists in the NBA may examine how various frequencies of yoga and weightlifting sessions each week affect a player's point total. With yoga and weightlifting sessions as the predictor variables and total points earned as the response variable, they could fit a multiple linear regression model. Here's another project that solves the same business problem, [Learn to Build a Polynomial Regression Model from Scratch](https://www.projectpro.io/project-use-case/polynomial-regression-model-in-python-from-scratch)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_nMzmrMWchJw",
      "metadata": {
        "id": "_nMzmrMWchJw"
      },
      "source": [
        "## **Interview Questions**\n",
        "\n",
        "* What is a decision tree and how does it work?\n",
        "\n",
        "* What is the difference between regression and classification decision trees?\n",
        "\n",
        "* What are the advantages and disadvantages of using decision trees for predictive modeling?\n",
        "\n",
        "* What is the difference between a decision tree and a random forest, and in what situations would you use one versus the other?\n",
        "\n",
        "* Explain the concept of impurity in a decision tree. How is impurity calculated?\n",
        "\n",
        "* Can you describe the process of growing a decision tree?\n",
        "\n",
        "* How do you prevent a decision tree from overfitting to the training data?\n",
        "\n",
        "* How would you handle missing data?\n",
        "\n",
        "* How do you choose the best split when building a decision tree?\n",
        "\n",
        "* What is pruning, and why might you use it when building a decision tree?\n",
        "\n",
        "* Can you walk me through the process of building a decision tree from scratch in Python?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M7pdnpDUllBw",
      "metadata": {
        "id": "M7pdnpDUllBw"
      },
      "source": [
        "\n",
        "<h1><center>Thank you for choosing ProjectPro!</center></h1>\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
